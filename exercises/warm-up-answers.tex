\section{Warm-up Exercises Answers}
\subsection{Probability Theory}
\paragraph{\questionref{q:setsprob}}
\begin{enumerate}[label=\alph*.]
    \item We can choose any representation denoting the events, e.g. using abstract symbols $\Omega=\{ \epsdice{1}, \epsdice{2}, \epsdice{3}, \epsdice{4}, \epsdice{5}, \epsdice{6} \}$. Alternatively, we can represent each of the outcomes as a number $\Omega = \{1, 2, 3, 4, 5, 6\}$.
    
    Following the latter notation, $A=\{3, 4\}$, and $A=\{1, 2, 5, 6\}$.

	    
    
	\item Length problem with sample space $\Omega=[0,1]$.
	
	$\neg A = [0, 0.3] \cup (0.4,1]$
    
    \item $P(\neg  A) = 1 - P(A)$
    
    Since $\neg A$ and $A$ are mutually exclusive: $A \cup \neg A = \Omega$ and $A \cap \neg A = \varnothing$.

By combining axiom 2 and 3: $P(A) + P(\neg A) = P(A \cup \neg A) = P(\Omega) = 1$

Thus: $P(\neg A) = 1 - P(A)$ 

    \item $P(\varnothing) = 0$, where $\varnothing$ is the empty set
    
Given the sample space, $\Omega$, its complementary is the empty set $\varnothing$. 

We use property (c) and axiom 2: $ P(\varnothing) = 1 - P(\Omega) = 1 - 1 = 0$.

    \item $0 \leq P(A) \leq 1$
    
We use property (c) and axiom 1. 

Consider an event $A$, where $P(A) \geq 0$ and $P(\neg A) \geq 0$ by axiom 1. 

Then, $P(\neg A) = 1 - P(A) \geq 0 \implies 1 \geq P(A)$.

By joining both inequalities, $0 \leq P(A) \leq 1$.

    \item $A \subseteq B \implies P(A) \leq P(B)$
    
    \textit{Hint:} Consider the following definition. $B\backslash  A = \{x\in B: x\notin A\}$

Assume $A \subseteq B$ and construct $B$ as the union of two disjoint sets: $B = B\backslash A \cup A$.

Then, $B\backslash A \cap A = \varnothing$ by definition of $B\backslash A$. By axiom 1, we have $P(B\backslash A) \geq 0$. 

Use axiom 3: $P(B) = P(B\backslash A) + P(A) \geq P(A) \implies P(A) \leq P(B)$.


    \item $P(A\cup B) = P(A) + P(B) - P(A\cap B)$.

Define the union $(A\cup B)$ in terms of two disjoint sets. $(A \cup B) = A \cup B\backslash A$, where $A \cap B\backslash A = \varnothing$. 

Use axiom 3: $P(A \cup B) = P(A) + P(B\backslash A)$.

To compute $P(B\backslash A)$, we define B in terms of A, and the union of two disjoint sets: $B = (B \cap A) \cup (B\backslash A)$, where $(B \cap A) \cap (B\backslash A) = \varnothing$ by definition. 

Use axiom 3 again: $P(B) = P(B \cap A) + P(B\backslash A) \implies P(B\backslash A) = P(B) - P(B \cap A)$.

Finally: $P(A \cup B) = P(A) + P(B\backslash A) = P(A) + P(B) - P(B \cap A)$.

    \item (\textbf{*}) if $\{A_i\}_{i=1}^\infty \subseteq \Omega \text{ and } A_{i-1} \subseteq A_{i}\quad \forall i>0$ then:
\[
P\left(\bigcup_{i=1}^{\infty} A_{i}\right) = \lim_{i\xrightarrow{}\infty} P(A_i)
\]
\textit{Hint:} Use axiom 3.

Let us define the following: $A := \bigcup_{i=1}^{\infty} A_{i}$. We would like to write $A$ in terms of disjoint sets to use axiom 3.
\begin{align}\label{eq:sets:disjoint-sets}
A_{i-1} \subseteq A_i \quad \forall i > 0 \implies A = \bigcup_{i=1}^{\infty} A_{i}\backslash A_{i-1}
\end{align}
where the expression holds if we have $A_{0} = \varnothing$. We regard \ref{eq:sets:disjoint-sets} as starting with $A_1$ and adding the new information from $A_2, A_3,\dots$ (e.g $A_2 \backslash A_1, A_3 \backslash A_2, \dots$). 
\begin{align}\label{eq:sets:axiom3-on-set}
P(A) = P\left(\bigcup_{i=1}^{\infty} A_{i}\backslash A_{i-1}\right) = \sum_{i=1}^\infty P(A_{i}\backslash A_{i-1})&&\text{(by axiom 3)}\\
P(A) = \sum_{i=1}^\infty P(A_{i}\backslash A_{i-1}) = \lim_{n\xrightarrow{}\infty} \sum_{i=1}^n P(A_{i}\backslash A_{i-1}) &&\text{(the infinite summation is a limit)}
\end{align}
From (f), we have $P(A_i) = P(A_{i}\backslash A_{i-1}) + P(A_{i-1}) \implies  P(A_{i}\backslash A_{i-1}) = P(A_i) - P(A_{i-1})$. Then,
\begin{align}
P(A) = \lim_{n\xrightarrow{}\infty} \sum_{i=1}^n P(A_i) - P(A_{i-1}) = \lim_{n\xrightarrow{}\infty} \bigg(\sum_{i=1}^n P(A_i) - \sum_{i=1}^{n-1} P(A_{i})\bigg) = \lim_{n\xrightarrow{}\infty} P(A_n)
\end{align}
where we used $P(A_0) = P(\varnothing) = 0$ from (d). 

In summary:
\begin{align}
P\left(\bigcup_{i=1}^{\infty} A_{i}\right) = P(A) = \lim_{i\xrightarrow{}\infty} P(A_i)
\end{align}

\end{enumerate}

\paragraph{\questionref{q:rv}}
\begin{enumerate}[label=\alph*.]
\item
We choose to represent the outcomes of two dice as integer tuples:
\begin{align*}
\Omega = \{
&(\dA,\dA), (\dA,\dB), (\dA,\dC), (\dA,\dD), (\dA,\dE), (\dA,\dF),  \\
&(\dB,\dA), (\dB,\dB), (\dB,\dC), (\dB,\dD), (\dB,\dE), (\dB,\dF),  \\
&(\dC,\dA), (\dC,\dB), (\dC,\dC), (\dC,\dD), (\dC,\dE), (\dC,\dF),  \\
&(\dD,\dA), (\dD,\dB), (\dD,\dC), (\dD,\dD), (\dD,\dE), (\dD,\dF),  \\
&(\dE,\dA), (\dE,\dB), (\dE,\dC), (\dE,\dD), (\dE,\dE), (\dE,\dF),  \\
&(\dF,\dA), (\dF,\dB), (\dF,\dC), (\dF,\dD), (\dF,\dE), (\dF,\dF) \}
\end{align*}
\item
We define random variables A and B to be:
\begin{align*}
A(s) =
\begin{cases}
1 &\text{, if } s \in \{ (\dA, \dA), (\dA,\dB), (\dA,\dC), (\dA,\dD), (\dA,\dE), (\dA,\dF) \} \\
2 &\text{, if } s \in \{ (\dB, \dA), (\dB,\dB), (\dB,\dC), (\dB,\dD), (\dB,\dE), (\dB,\dF) \} \\
3 &\text{, if } s \in \{ (\dC, \dA), (\dC,\dB), (\dC,\dC), (\dC,\dD), (\dC,\dE), (\dC,\dF) \} \\
4 &\text{, if } s \in \{ (\dD, \dA), (\dD,\dB), (\dD,\dC), (\dD,\dD), (\dD,\dE), (\dD,\dF) \} \\
5 &\text{, if } s \in \{ (\dE, \dA), (\dE,\dB), (\dE,\dC), (\dE,\dD), (\dE,\dE), (\dE,\dF) \} \\
5 &\text{, if } s \in \{ (\dF, \dA), (\dF,\dB), (\dF,\dC), (\dF,\dD), (\dF,\dE), (\dF,\dF) \}
\end{cases} \\
B(s) =
\begin{cases}
1 &\text{, if } s \in \{ (\dA,\dA), (\dB,\dA), (\dC,\dA), (\dD,\dA), (\dE,\dA), (\dF,\dA) \} \\
2 &\text{, if } s \in \{ (\dA,\dB), (\dB,\dB), (\dC,\dB), (\dD,\dB), (\dE,\dB), (\dF,\dB) \} \\
3 &\text{, if } s \in \{ (\dA,\dC), (\dB,\dC), (\dC,\dC), (\dD,\dC), (\dE,\dC), (\dF,\dC) \} \\
4 &\text{, if } s \in \{ (\dA,\dD), (\dB,\dD), (\dC,\dD), (\dD,\dD), (\dE,\dD), (\dF,\dD) \} \\
5 &\text{, if } s \in \{ (\dA,\dE), (\dB,\dE), (\dC,\dE), (\dD,\dE), (\dE,\dE), (\dF,\dE) \} \\
5 &\text{, if } s \in \{ (\dA,\dF), (\dB,\dF), (\dC,\dF), (\dD,\dF), (\dE,\dF), (\dF,\dF) \}
\end{cases}
\end{align*}
We can find the PDFs by counting the number of occurrences in $\Omega$. For instance:
\begin{align*}
p_A(3) = \frac{|\{ (\dC, \dA), (\dC,\dB), (\dC,\dC), (\dC,\dD), (\dC,\dE), (\dC,\dF) \}|}{|\Omega|} = \frac{6}{36} = \frac{1}{6}
\end{align*}
Repeating this for all outcomes gives us the full PDFs:
\begin{align*}
\begin{split}
p_A(x) = \begin{cases}
\frac{1}{6} &\text{, if } x = 1\\
\frac{1}{6} &\text{, if } x = 2\\
\frac{1}{6} &\text{, if } x = 3\\
\frac{1}{6} &\text{, if } x = 4\\
\frac{1}{6} &\text{, if } x = 5\\
\frac{1}{6} &\text{, if } x = 6\\
0 &\text{, otherwise } \\
\end{cases}
\end{split}\text{, }
\begin{split}
p_B(x) = \begin{cases}
\frac{1}{6} &\text{, if } x = 1\\
\frac{1}{6} &\text{, if } x = 2\\
\frac{1}{6} &\text{, if } x = 3\\
\frac{1}{6} &\text{, if } x = 4\\
\frac{1}{6} &\text{, if } x = 5\\
\frac{1}{6} &\text{, if } x = 6\\
0 &\text{, otherwise } \\
\end{cases}
\end{split}
\end{align*}
\item
To show independence of $A$ and $B$ we must show that $p(A \cap B) = p(A)p(B)$.
We have that all outcomes have equal probability $\frac{1}{|\Omega|} = \frac{1}{36}$ and therefore:
\begin{align*}
p(A \cap B) = \frac{1}{36} = \frac{1}{6} \cdot \frac{1}{6} = p(A) p(B)
\end{align*}
\item
We can define a random variable $C = A + B$
\begin{align*}
C(s) =
\begin{cases}
2 &\text{, if } s \in \{ (\dA, \dA) \} \\
3 &\text{, if } s \in \{ (\dA, \dB), (\dB,\dA) \} \\
4 &\text{, if } s \in \{ (\dA, \dC), (\dB,\dB), (\dC,\dA) \} \\
5 &\text{, if } s \in \{ (\dA, \dD), (\dB,\dC), (\dC,\dB), (\dD,\dA) \} \\
6 &\text{, if } s \in \{ (\dA, \dE), (\dB,\dD), (\dC,\dC), (\dD,\dB), (\dE,\dA) \} \\
7 &\text{, if } s \in \{ (\dA, \dF), (\dB,\dE), (\dC,\dD), (\dD,\dC), (\dE,\dB), (\dF,\dA) \} \\
8 &\text{, if } s \in \{ (\dB, \dF), (\dC,\dE), (\dD,\dD), (\dE,\dC), (\dF,\dB) \} \\
9 &\text{, if } s \in \{ (\dC, \dF), (\dD,\dE), (\dE,\dD), (\dF,\dC) \} \\
10 &\text{, if } s \in \{ (\dD, \dF), (\dE,\dE), (\dF,\dD) \} \\
11 &\text{, if } s \in \{ (\dE, \dF), (\dF,\dE) \} \\
12 &\text{, if } s \in \{ (\dF, \dF) \} \\
\end{cases}
\end{align*}
Then the PDF $p_C$ becomes:
\begin{align*}
p_C(x) = \begin{cases}
\frac{1}{36} = \frac{1}{36} &\text{, if } x = 2\\
\frac{2}{36} = \frac{1}{18} &\text{, if } x = 3\\
\frac{3}{36} = \frac{1}{12} &\text{, if } x = 4\\
\frac{4}{36} = \frac{1}{9}  &\text{, if } x = 5\\
\frac{5}{36} = \frac{5}{36} &\text{, if } x = 6\\
\frac{6}{36} = \frac{1}{6}  &\text{, if } x = 7\\
\frac{5}{36} = \frac{5}{36} &\text{, if } x = 8\\
\frac{3}{36} = \frac{1}{9}  &\text{, if } x = 9\\
\frac{3}{36} = \frac{1}{12} &\text{, if } x = 10\\
\frac{2}{36} = \frac{1}{18} &\text{, if } x = 11\\
\frac{1}{36} = \frac{1}{36} &\text{, if } x = 12\\
0 &\text{, otherwise } \\
\end{cases}
\end{align*}
which can be rewritten in more compact form:
\begin{align*}
p_C(x) = \begin{cases}
\frac{6 - |x-6|}{36} &\text{, if } x = \{ 2, 3, \ldots, 12\}\\
0 &\text{, otherwise } \\
\end{cases}
\end{align*}
\end{enumerate}


\paragraph{\questionref{q:stats-term}}
\begin{enumerate}[label=\alph*.]
\item A statistic is a function that is computed from data. For example, take a data set $X = \{x_1, x_2, x_3, \dots\}$ where we compute the empirical mean $\bar X = \frac{1}{|X|}\sum_n x_n$.
\item An estimator is a function of data that tries to estimate an unknown quantity. Estimators are statistics. Some statistics are also estimators. For example, if we have some data set from that is sampled from some unknown density $p(x)$, then its mean is unknown, and $\bar X$ is an estimator of it.
\item A consistent estimator finds the correct value of the unknown quantity if the dataset grows to infinity. We will prove that $\bar X$ is a consistent estimate of $\int p(x) x \calcd x$ later on in the course.
\item A sample from a random variable is an outcome of the random experiment it represents. For example, you can have a random variable representing the outcome of a coin toss. A sample from it would be heads or tails. We sampled a random variable independently many times, then the outcomes would occur with the frequency specified by the probability distribution of the random variable. Thinking about sampling outcomes from a random variable is often a helpful conceptual technique to think about randomness.
\end{enumerate}

\subsection{Linear Algebra}

\paragraph{Question \ref{q:dot_product}}
$\x^\top \y = 1 \times 0 + (-2) \times 4 + 5 \times (-3) + (-1) \times 7 = 0 + (-8) + (-15) + (-7) = -30$.

\paragraph{Question \ref{q:matrix_product}}
$\y = (24, -14, -12)^\top$, $|| \x ||_2 = \sqrt{23}$, $|| \y ||_2 = \sqrt{916}$.

Note that by definition the $\ell_2$ norm of a vector is $|| \x ||_2 = \sqrt{\x^\top \x}$.

\paragraph{Question \ref{q:basis}} 1, 2, 3. 

A set of vectors $\{\mathbf{b}_1, ..., \mathbf{b}_K \}$ with $\mathbf{b}_k \in \mathbb{R}^d$ can form a basis of $\mathbb{R}^d$ iff $K \geq d$ and there exists a subset of $d$ vectors within the set, such that they are orthogonal to each other.

\paragraph{Question \ref{q:span}} 2, 5. 

A point $\x \in \mathbb{R}^d$ is in $span(\{\mathbf{b}_1, ..., \mathbf{b}_K \})$ with $\mathbf{b}_k \in \mathbb{R}^d$ iff we can find $a_1, ..., a_K \in \mathbb{R}$ such that $\x = \sum_{k=1}^K a_k \mathbf{b}_k$.

\paragraph{Question \ref{q:rotation_matrix}} The rotation matrix is 
\begin{equation*}
    \begin{pmatrix}
    \cos{\frac{\pi}{4}} & -\sin{\frac{\pi}{4}} \\
    \sin{\frac{\pi}{4}} & \cos{\frac{\pi}{4}}
    \end{pmatrix}.
\end{equation*}

\paragraph{Question \ref{q:linear_equations}}

a) The matrix $A$ and vector $\mathbf{b}$ are
\begin{equation*}
A = \begin{pmatrix}
1 & 2 & 0 \\
3 & 2 & 4 \\
-2 & 1 & -2
\end{pmatrix}, \quad \mathbf{b} = (2, 5, 1)^\top.
\end{equation*}

b) The inverse of $A$ is 
\begin{equation*}
A^{-1} = \begin{pmatrix}
2/3 & -1/3 & -2/3 \\
1/6 & 1/6 & 1/3 \\
7/12 & 5/12 & 1/3
\end{pmatrix}.
\end{equation*}
Therefore we have $\x = A^{-1} \mathbf{b} = (-1, 3/2, 43/12)^\top$.

c) $\text{rank}(A) = 3$: as $A$ is invertible, it must have full rank.

\paragraph{Question \ref{q:eigen_decomp}}

a) When $A$ is symmetric, then $A = Q \Lambda Q^\top$, and $\x^\top A \x = \x^\top Q \Lambda Q^\top \x = (Q^\top \x)^\top \Lambda (Q^\top \x)$. As $Q$ is an orthonormal matrix, we have $\x \rightarrow Q^\top \x$ a one-to-one mapping. Therefore we have
$$\x^\top A \x = \bm{z}^\top \Lambda \bm{z} = \sum_{i=1}^d \lambda_i z_i^2, \quad \bm{z} = (z_1, ..., z_d)^\top = Q^\top \x.$$
Therefore $\x^\top A \x \geq 0 \Leftrightarrow \sum_{i=1}^d \lambda_i z_i^2 \geq 0$. This is true for any $\x \in \mathbb{R}^{d \times 1}$ if and only if $\lambda_i \geq 0$ for all $i = 1,..., d$.

b) We use the permuation invariance property of matrix trace to show the result:
$$Tr(A) = Tr(Q \Lambda Q^{-1}) = Tr(Q^{-1} Q \Lambda) = Tr(\Lambda) = \sum_{i=1}^d \lambda_i.$$

c) We use the product rule of matrix determinant to show the result:
$$det(A) = det(Q \Lambda Q^{-1}) = det(Q) det(\Lambda) det(Q^{-1}) = det(Q) det(\Lambda) det(Q)^{-1} = det(\Lambda) = \prod_{i=1}^d \lambda_i.$$

d) Let us assume the statement is false, i.e., there exists a solution $\lambda^* \neq \lambda_i, \forall i = 1, ..., d$ for the equation $A \bm{q} = \lambda \bm{q}, \bm{q} \neq 0$. Then we can rewrite the equation as
$$A \bm{q} = \lambda^* \bm{q} \quad \Rightarrow \quad (A - \lambda^* I) \bm{q} = \bm{0} \quad \Rightarrow \quad Q (\Lambda - \lambda^* I) Q^{-1} \bm{q} = 0.$$
By definition, the column vectors of $Q$ forms a basis of $\mathbb{R}^d$. Notice that the diagonal entries of $\Lambda - \lambda^* I$ are non-zero as we assume $\lambda^* \neq \lambda_i$. This indicates a contradiction to the assumption of $\bm{q} \neq 0$:
$$Q (\Lambda - \lambda^* I) Q^{-1} \bm{q} = 0 \quad \Rightarrow \quad Q^{-1}\bm{q} = \bm{0} \quad \Rightarrow \quad \bm{q} = \bm{0}.$$

